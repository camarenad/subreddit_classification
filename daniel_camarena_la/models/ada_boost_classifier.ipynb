{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA BOOST \n",
    "Boosting is an ensemble model that creates a strong classifier from a number of weaker clasifiers. An Ada or adaptive boost is sensitive to noise and outliers but is less susceptible to overfitting data.I used the boost in conjuntion with a decision tree classifier. Given the nature of this style of modeling, my ability to grid search parameters was limited by my computing power. Some of the models took 3-6 hours to fit.\n",
    "\n",
    "I did run tune a handful of hyperparameters before finding an acceptable fit. My baseline score is %50. I made a concious choice to keep my classes balanced. It scored an accuracy of about 68 percent which seems reasonable with volume of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'author_cakeday', 'author_flair_background_color',\n",
       "       'author_flair_css_class', 'author_flair_richtext',\n",
       "       'author_flair_template_id', 'author_flair_text',\n",
       "       'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
       "       'author_id', 'body', 'created_utc', 'distinguished', 'edited', 'id',\n",
       "       'link_id', 'no_follow', 'parent_id', 'permalink', 'retrieved_on',\n",
       "       'rte_mode', 'score', 'send_replies', 'stickied', 'subreddit',\n",
       "       'subreddit_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_pickle('../assets/combined.pkl')\n",
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../assets/X_train.pkl')\n",
    "X_test = pd.read_pickle('../assets/X_test.pkl')\n",
    "y_train = pd.read_pickle('../assets/y_train.pkl')\n",
    "y_test = pd.read_pickle('../assets/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Pipeline\n",
    "The tfidf vec is breaking the words into single tokens and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english',ngram_range=(1,2))),\n",
    "    ('ada',AdaBoostClassifier()),\n",
    "    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the parameter grid\n",
    "While I did take the time to play with various min and max df settings. I noticed in my eda and the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid =  {\n",
    "    'tfidf__min_df': np.arange(1,4,2),\n",
    "    'tfidf__max_df': [.95, .98, 1.0],\n",
    "    'ada__n_estimators':[10,20,30,60,70,80]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid=param_grid,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 139.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...m='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__min_df': array([1, 3]), 'tfidf__max_df': [0.95, 0.98, 1.0], 'ada__n_estimators': [10, 20, 30, 60, 70, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the training and testing set\n",
    "This model beat my baseline but just barely. The amount of time of time it took to fit each model made this an undesirable model for me.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6425261982252393"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114607,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Libertarian            0.527031\n",
       "LateStageCapitalism    0.472969\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['subreddit'].value_counts()/combined.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6412715863791724"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Estimators\n",
    "The min and max df values I used ended up being the same on all of my models. I did experiement with many different variations each of which was more expensive computationally until I settled into a maxdf of 95%. The results of teh gridsearch also indicate that 80 was a good number of estimators. I would have liked to tune more hyperparameters but was limited by my lack of serious processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...m='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=80, random_state=None))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feeding data into the pipeline\n",
    "All of my models suffered from low sensitivity this one was no exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Libertarian'], dtype='<U19')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict([\"Worth noting the evolution of Trump's Syria policy is very similar to what happened with Afghanistan. In both places, Trump instinctively wanted to wind down U.S. military presence -- foreign policy aides convinced him not only to maintain status quo, but expand the mission.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../assets/ada_boost.pkl','wb+') as f:\n",
    "    pickle.dump(gs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = pickle.load( open( \"../assets/ada_boost.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada__n_estimators': 80, 'tfidf__max_df': 0.95, 'tfidf__min_df': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
